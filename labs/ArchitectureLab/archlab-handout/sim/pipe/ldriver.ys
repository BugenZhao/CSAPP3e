#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#/* $begin ncopy-ys */
##################################################################
# ncopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
# Include your name and ID here.
#
# Describe how and why you modified the baseline code.
#
##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
ncopy:

##################################################################
# You can modify this portion

#	xorq 	%rax, %rax		# count = 0;
	iaddq	$-8, %rdx
	jg		loop0			# len >= 2

r_test:
	xorq	%r8, %r8
	iaddq	$4, %rdx		# [-8,0] -> [-4,4]	(+4)
	jl		r_test_l
	jg		r_test_r
	je		r4

r_test_l:
	iaddq	$2, %rdx		# [-4,-1] -> [-2,1]	(+2)
	je		r2
	jg		r3
	iaddq	$2, %rdx		# [-2,-1] -> [0,1]	(+0)
	jg		r1
	ret

r_test_r:
	iaddq	$-2, %rdx		# [1,4] -> [-1,2]	(+6)
	je		r6
	jl		r5
	iaddq	$-2, %rdx		# [1,2] -> [-1,0]	(+8)
	jl		r7

r8:
	mrmovq	56(%rdi), %r8
r8b:
	rmmovq	%r8, 56(%rsi)
r7:
	andq	%r8, %r8
	mrmovq	48(%rdi), %r8
	jle		r7b				# 56(%rdi)
	iaddq	$1, %rax
r7b:
	rmmovq	%r8, 48(%rsi)
r6:
	andq	%r8, %r8
	mrmovq	40(%rdi), %r8
	jle		r6b
	iaddq	$1, %rax
r6b:
	rmmovq	%r8, 40(%rsi)
r5:
	andq	%r8, %r8
	mrmovq	32(%rdi), %r8
	jle		r5b
	iaddq	$1, %rax
r5b:
	rmmovq	%r8, 32(%rsi)
r4:
	andq	%r8, %r8
	mrmovq	24(%rdi), %r8
	jle		r4b
	iaddq	$1, %rax
r4b:
	rmmovq	%r8, 24(%rsi)
r3:
	andq	%r8, %r8
	mrmovq	16(%rdi), %r8
	jle		r3b
	iaddq	$1, %rax
r3b:
	rmmovq	%r8, 16(%rsi)
r2:
	andq	%r8, %r8
	mrmovq	8(%rdi), %r8
	jle		r2b
	iaddq	$1, %rax
r2b:
	rmmovq	%r8, 8(%rsi)
r1:
	andq	%r8, %r8
	mrmovq	(%rdi), %r8
	jle		r1b
	iaddq	$1, %rax
r1b:
	rmmovq	%r8, (%rsi)
	andq	%r8, %r8
r1c:
	jle		Done
	iaddq	$1, %rax
	ret



loop0:
	mrmovq	(%rdi), %r8
	iaddq	$72, %rdi
	rmmovq	%r8, (%rsi)
	andq	%r8, %r8
	jle		loop1
	iaddq	$1, %rax
loop1:
	mrmovq	-64(%rdi), %r8
	iaddq	$72, %rsi
	rmmovq	%r8, -64(%rsi)
	andq	%r8, %r8
	jle 	loop2
	iaddq	$1, %rax
loop2:
	mrmovq	-56(%rdi), %r8
	mrmovq	-48(%rdi), %r9
	rmmovq	%r8, -56(%rsi)
	andq	%r8, %r8
	jle 	loop3
	iaddq	$1, %rax
loop3:
	mrmovq	-40(%rdi), %r8
	rmmovq	%r9, -48(%rsi)
	andq	%r9, %r9
	jle 	loop4
	iaddq	$1, %rax
loop4:
	mrmovq	-32(%rdi), %r9
	rmmovq	%r8, -40(%rsi)
	andq	%r8, %r8
	jle 	loop5
	iaddq	$1, %rax
loop5:
	mrmovq	-24(%rdi), %r8
	rmmovq	%r9, -32(%rsi)
	andq	%r9, %r9
	jle 	loop6
	iaddq	$1, %rax
loop6:
	mrmovq	-16(%rdi), %r9
	rmmovq	%r8, -24(%rsi)
	andq	%r8, %r8
	jle 	loop7
	iaddq	$1, %rax
loop7:
	mrmovq	-8(%rdi), %r8
	rmmovq	%r9, -16(%rsi)
	andq	%r9, %r9
	jle 	loop8
	iaddq	$1, %rax
loop8:
	rmmovq	%r8, -8(%rsi)
	andq	%r8, %r8
	jle		loop_end
	iaddq	$1, %rax

loop_end:
	iaddq	$-9, %rdx
	jg		loop0
	jmp		r_test


##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
	ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad -1
	.quad -2
	.quad -3
	.quad -4
	.quad 5
	.quad -6
	.quad 7
	.quad 8
	.quad -9
	.quad -10
	.quad -11
	.quad -12
	.quad 13
	.quad 14
	.quad -15
	.quad -16
	.quad 17
	.quad -18
	.quad -19
	.quad 20
	.quad 21
	.quad -22
	.quad 23
	.quad 24
	.quad -25
	.quad 26
	.quad 27
	.quad 28
	.quad -29
	.quad -30
	.quad 31
	.quad 32
	.quad -33
	.quad 34
	.quad 35
	.quad 36
	.quad -37
	.quad -38
	.quad -39
	.quad 40
	.quad -41
	.quad 42
	.quad 43
	.quad -44
	.quad -45
	.quad -46
	.quad 47
	.quad -48
	.quad 49
	.quad -50
	.quad -51
	.quad 52
	.quad 53
	.quad 54
	.quad -55
	.quad 56
	.quad 57
	.quad -58
	.quad -59
	.quad -60
	.quad 61
	.quad 62
	.quad 63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:

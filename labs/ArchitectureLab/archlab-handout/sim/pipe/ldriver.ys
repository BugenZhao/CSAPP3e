#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#/* $begin ncopy-ys */
##################################################################
# ncopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
# Include your name and ID here.
#
# Describe how and why you modified the baseline code.
#
##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
ncopy:

##################################################################
# You can modify this portion

#	xorq 	%rax, %rax		# count = 0;
	iaddq	$-8, %rdx
	jg		loop0			# len >= 2

r_test:
	iaddq	$4, %rdx		# [-8,0] -> [-4,4]	(+4)
	jl		r_test_l
	jg		r_test_r
	jmp		r4

r_test_l:
	iaddq	$2, %rdx		# [-4,-1] -> [-2,1]	(+2)
	je		r2
	jg		r3
	iaddq	$2, %rdx		# [-2,-1] -> [0,1]	(+0)
	jg		r1
	ret

r_test_r:
	iaddq	$-2, %rdx		# [1,4] -> [-1,2]	(+6)
	je		r6
	jl		r5
	iaddq	$-2, %rdx		# [1,2] -> [-1,0]	(+8)
	jl		r7

r8:
	mrmovq	56(%rdi), %r10
r8b:
	rmmovq	%r10, 56(%rsi)
r7:
	# %r10 was never used before. Directly jumping to r7 means %r10 = 0.
	andq	%r10, %r10
	mrmovq	48(%rdi), %r10
	jle		r7b				# 56(%rdi)
	iaddq	$1, %rax
r7b:
	rmmovq	%r10, 48(%rsi)
r6:
	andq	%r10, %r10
	mrmovq	40(%rdi), %r10
	jle		r6b
	iaddq	$1, %rax
r6b:
	rmmovq	%r10, 40(%rsi)
r5:
	andq	%r10, %r10
	mrmovq	32(%rdi), %r10
	jle		r5b
	iaddq	$1, %rax
r5b:
	rmmovq	%r10, 32(%rsi)
r4:
	andq	%r10, %r10
	mrmovq	24(%rdi), %r10
	jle		r4b
	iaddq	$1, %rax
r4b:
	rmmovq	%r10, 24(%rsi)
r3:
	andq	%r10, %r10
	mrmovq	16(%rdi), %r10
	jle		r3b
	iaddq	$1, %rax
r3b:
	rmmovq	%r10, 16(%rsi)
r2:
	andq	%r10, %r10
	mrmovq	8(%rdi), %r10
	jle		r2b
	iaddq	$1, %rax
r2b:
	rmmovq	%r10, 8(%rsi)
r1:
	andq	%r10, %r10
	mrmovq	(%rdi), %r10
	jle		r1b
	iaddq	$1, %rax
r1b:
	rmmovq	%r10, (%rsi)
r1c:
	andq	%r10, %r10
	jle		Done
	iaddq	$1, %rax
	ret



loop0:
	mrmovq	(%rdi), %r8
	iaddq	$72, %rdi
	rmmovq	%r8, (%rsi)
	andq	%r8, %r8
	jle		loop1
	iaddq	$1, %rax
loop1:
	mrmovq	-64(%rdi), %r8
	iaddq	$72, %rsi
	rmmovq	%r8, -64(%rsi)
	andq	%r8, %r8
	jle 	loop2
	iaddq	$1, %rax
loop2:
	mrmovq	-56(%rdi), %r8
	mrmovq	-48(%rdi), %r9
	rmmovq	%r8, -56(%rsi)
	andq	%r8, %r8
	jle 	loop3
	iaddq	$1, %rax
loop3:
	mrmovq	-40(%rdi), %r8
	rmmovq	%r9, -48(%rsi)
	andq	%r9, %r9
	jle 	loop4
	iaddq	$1, %rax
loop4:
	mrmovq	-32(%rdi), %r9
	rmmovq	%r8, -40(%rsi)
	andq	%r8, %r8
	jle 	loop5
	iaddq	$1, %rax
loop5:
	mrmovq	-24(%rdi), %r8
	rmmovq	%r9, -32(%rsi)
	andq	%r9, %r9
	jle 	loop6
	iaddq	$1, %rax
loop6:
	mrmovq	-16(%rdi), %r9
	rmmovq	%r8, -24(%rsi)
	andq	%r8, %r8
	jle 	loop7
	iaddq	$1, %rax
loop7:
	mrmovq	-8(%rdi), %r8
	rmmovq	%r9, -16(%rsi)
	andq	%r9, %r9
	jle 	loop8
	iaddq	$1, %rax
loop8:
	rmmovq	%r8, -8(%rsi)
	andq	%r8, %r8
	jle		loop_end
	iaddq	$1, %rax

loop_end:
	iaddq	$-9, %rdx
	jg		loop0
	jmp		r_test


##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
	ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad -1
	.quad 2
	.quad -3
	.quad -4
	.quad 5
	.quad -6
	.quad 7
	.quad 8
	.quad 9
	.quad -10
	.quad 11
	.quad -12
	.quad 13
	.quad 14
	.quad -15
	.quad 16
	.quad 17
	.quad 18
	.quad 19
	.quad -20
	.quad -21
	.quad 22
	.quad 23
	.quad 24
	.quad 25
	.quad -26
	.quad 27
	.quad 28
	.quad 29
	.quad 30
	.quad -31
	.quad -32
	.quad -33
	.quad -34
	.quad 35
	.quad 36
	.quad -37
	.quad 38
	.quad -39
	.quad -40
	.quad -41
	.quad -42
	.quad 43
	.quad -44
	.quad -45
	.quad -46
	.quad 47
	.quad -48
	.quad 49
	.quad 50
	.quad 51
	.quad -52
	.quad -53
	.quad -54
	.quad -55
	.quad -56
	.quad -57
	.quad 58
	.quad 59
	.quad 60
	.quad -61
	.quad -62
	.quad -63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
